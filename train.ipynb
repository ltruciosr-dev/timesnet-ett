{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TimesNet Training on ETT Datasets\n",
    "\n",
    "This notebook trains TimesNet on all 4 ETT datasets (ETTh1, ETTh2, ETTm1, ETTm2) with different prediction horizons.\n",
    "\n",
    "**Datasets:**\n",
    "- ETTh1, ETTh2: Hourly data\n",
    "- ETTm1, ETTm2: 15-minute data\n",
    "\n",
    "**Configuration:**\n",
    "- Input length: 96 (fixed, as in paper)\n",
    "- Prediction horizons: {24, 48, 96, 192, 336, 720}\n",
    "- Total experiments: 4 datasets × 6 horizons = 24 models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('./src')\n",
    "\n",
    "from train import TimesNetTrainer\n",
    "from evaluate import evaluate_and_save_results, plot_training_curves\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base paths\n",
    "ROOT_PATH = './ETDataset/ETT-small/'\n",
    "CHECKPOINT_BASE = './checkpoints'\n",
    "RESULTS_DIR = './results'\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(CHECKPOINT_BASE, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Datasets\n",
    "DATASETS = ['ETTh1', 'ETTh2', 'ETTm1', 'ETTm2']\n",
    "\n",
    "# Fixed input length (as in paper)\n",
    "SEQ_LEN = 96\n",
    "\n",
    "# Prediction horizons\n",
    "PRED_LENS = [24, 48, 96, 192, 336, 720]\n",
    "\n",
    "# Model hyperparameters (from paper)\n",
    "MODEL_CONFIGS = {\n",
    "    'ETTh1': {'d_model': 16, 'd_ff': 32},\n",
    "    'ETTh2': {'d_model': 16, 'd_ff': 32},\n",
    "    'ETTm1': {'d_model': 32, 'd_ff': 64},\n",
    "    'ETTm2': {'d_model': 32, 'd_ff': 64},\n",
    "}\n",
    "\n",
    "# Training config\n",
    "TRAIN_CONFIG = {\n",
    "    'enc_in': 7,\n",
    "    'c_out': 7,\n",
    "    'top_k': 5,\n",
    "    'e_layers': 2,\n",
    "    'num_kernels': 6,\n",
    "    'dropout': 0.1,\n",
    "    'embed': 'fixed',\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.0001,\n",
    "    'train_epochs': 10,\n",
    "    'patience': 3,\n",
    "    'lradj': 'type1',\n",
    "    'use_amp': False,\n",
    "    'num_workers': 0,\n",
    "}\n",
    "\n",
    "print(f\"✓ Configuration set\")\n",
    "print(f\"  - Datasets: {DATASETS}\")\n",
    "print(f\"  - Input length: {SEQ_LEN}\")\n",
    "print(f\"  - Prediction horizons: {PRED_LENS}\")\n",
    "print(f\"  - Total experiments: {len(DATASETS) * len(PRED_LENS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_model(dataset_name, pred_len):\n",
    "    \"\"\"\n",
    "    Train a single TimesNet model\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: Dataset name (e.g., 'ETTh1')\n",
    "        pred_len: Prediction horizon\n",
    "    \n",
    "    Returns:\n",
    "        dict: Training results\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"Training: {dataset_name} | seq_len={SEQ_LEN} → pred_len={pred_len}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Create config\n",
    "    config = {\n",
    "        'root_path': ROOT_PATH,\n",
    "        'data_path': f'{dataset_name}.csv',\n",
    "        'seq_len': SEQ_LEN,\n",
    "        'pred_len': pred_len,\n",
    "        'checkpoints': f'{CHECKPOINT_BASE}/{dataset_name}_{SEQ_LEN}_{pred_len}',\n",
    "        **TRAIN_CONFIG,\n",
    "        **MODEL_CONFIGS[dataset_name]\n",
    "    }\n",
    "    \n",
    "    # Create checkpoint directory\n",
    "    os.makedirs(config['checkpoints'], exist_ok=True)\n",
    "    \n",
    "    # Train\n",
    "    trainer = TimesNetTrainer(config)\n",
    "    train_losses, val_losses = trainer.train()\n",
    "    \n",
    "    # Test\n",
    "    test_results = trainer.test()\n",
    "    \n",
    "    # Save training curves\n",
    "    curve_path = f'{RESULTS_DIR}/{dataset_name}_{SEQ_LEN}_{pred_len}_curves.png'\n",
    "    plot_training_curves(train_losses, val_losses, save_path=curve_path)\n",
    "    \n",
    "    # Prepare results\n",
    "    results = {\n",
    "        'dataset': dataset_name,\n",
    "        'seq_len': SEQ_LEN,\n",
    "        'pred_len': pred_len,\n",
    "        'd_model': config['d_model'],\n",
    "        'd_ff': config['d_ff'],\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'test_mse': test_results['mse'],\n",
    "        'test_mae': test_results['mae'],\n",
    "        'test_rmse': test_results['rmse'],\n",
    "        'final_epoch': len(train_losses),\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n✓ Completed: {dataset_name}_{SEQ_LEN}_{pred_len}\")\n",
    "    print(f\"  - Test MSE: {test_results['mse']:.6f}\")\n",
    "    print(f\"  - Test MAE: {test_results['mae']:.6f}\")\n",
    "    print(f\"  - Test RMSE: {test_results['rmse']:.6f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all results\n",
    "all_results = []\n",
    "\n",
    "# Train all combinations\n",
    "total_experiments = len(DATASETS) * len(PRED_LENS)\n",
    "current_experiment = 0\n",
    "\n",
    "for dataset_name in DATASETS:\n",
    "    for pred_len in PRED_LENS:\n",
    "        current_experiment += 1\n",
    "        \n",
    "        print(f\"\\n{'#'*70}\")\n",
    "        print(f\"# Experiment {current_experiment}/{total_experiments}\")\n",
    "        print(f\"{'#'*70}\")\n",
    "        \n",
    "        try:\n",
    "            results = train_single_model(dataset_name, pred_len)\n",
    "            all_results.append(results)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Error training {dataset_name}_{SEQ_LEN}_{pred_len}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ ALL TRAINING COMPLETED!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for easy analysis\n",
    "results_df = pd.DataFrame([{\n",
    "    'dataset': r['dataset'],\n",
    "    'seq_len': r['seq_len'],\n",
    "    'pred_len': r['pred_len'],\n",
    "    'd_model': r['d_model'],\n",
    "    'd_ff': r['d_ff'],\n",
    "    'test_mse': r['test_mse'],\n",
    "    'test_mae': r['test_mae'],\n",
    "    'test_rmse': r['test_rmse'],\n",
    "    'final_epoch': r['final_epoch']\n",
    "} for r in all_results])\n",
    "\n",
    "# Save to CSV\n",
    "csv_path = f'{RESULTS_DIR}/all_results.csv'\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "print(f\"✓ Results saved to {csv_path}\")\n",
    "\n",
    "# Save detailed results (with training curves) to JSON\n",
    "json_path = f'{RESULTS_DIR}/all_results_detailed.json'\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "print(f\"✓ Detailed results saved to {json_path}\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results by Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by dataset\n",
    "for dataset in DATASETS:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{dataset} Results\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    dataset_results = results_df[results_df['dataset'] == dataset]\n",
    "    display(dataset_results[['pred_len', 'test_mse', 'test_mae', 'test_rmse', 'final_epoch']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot MSE vs Prediction Horizon for each dataset\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, dataset in enumerate(DATASETS):\n",
    "    ax = axes[idx]\n",
    "    dataset_results = results_df[results_df['dataset'] == dataset].sort_values('pred_len')\n",
    "    \n",
    "    ax.plot(dataset_results['pred_len'], dataset_results['test_mse'], \n",
    "            marker='o', linewidth=2, markersize=8, label='MSE')\n",
    "    ax.plot(dataset_results['pred_len'], dataset_results['test_mae'], \n",
    "            marker='s', linewidth=2, markersize=8, label='MAE')\n",
    "    \n",
    "    ax.set_xlabel('Prediction Horizon', fontsize=12)\n",
    "    ax.set_ylabel('Error', fontsize=12)\n",
    "    ax.set_title(f'{dataset} - Error vs Prediction Horizon', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_DIR}/error_vs_horizon.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Plot saved to {RESULTS_DIR}/error_vs_horizon.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of MSE across datasets and horizons\n",
    "pivot_mse = results_df.pivot(index='dataset', columns='pred_len', values='test_mse')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(pivot_mse, annot=True, fmt='.4f', cmap='YlOrRd', \n",
    "            cbar_kws={'label': 'Test MSE'}, linewidths=0.5)\n",
    "plt.title('Test MSE: Dataset vs Prediction Horizon', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Prediction Horizon', fontsize=12)\n",
    "plt.ylabel('Dataset', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_DIR}/mse_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Heatmap saved to {RESULTS_DIR}/mse_heatmap.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of MAE across datasets and horizons\n",
    "pivot_mae = results_df.pivot(index='dataset', columns='pred_len', values='test_mae')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(pivot_mae, annot=True, fmt='.4f', cmap='YlGnBu', \n",
    "            cbar_kws={'label': 'Test MAE'}, linewidths=0.5)\n",
    "plt.title('Test MAE: Dataset vs Prediction Horizon', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Prediction Horizon', fontsize=12)\n",
    "plt.ylabel('Dataset', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_DIR}/mae_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Heatmap saved to {RESULTS_DIR}/mae_heatmap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\"*70)\n",
    "print(\"STATISTICAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nOverall Statistics:\")\n",
    "print(results_df[['test_mse', 'test_mae', 'test_rmse']].describe())\n",
    "\n",
    "print(\"\\nBest Results (by MSE):\")\n",
    "best_mse = results_df.loc[results_df.groupby('dataset')['test_mse'].idxmin()]\n",
    "display(best_mse[['dataset', 'pred_len', 'test_mse', 'test_mae', 'test_rmse']])\n",
    "\n",
    "print(\"\\nBest Results (by MAE):\")\n",
    "best_mae = results_df.loc[results_df.groupby('dataset')['test_mae'].idxmin()]\n",
    "display(best_mae[['dataset', 'pred_len', 'test_mse', 'test_mae', 'test_rmse']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with Paper Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper results (from TimesNet paper Table 1)\n",
    "paper_results = {\n",
    "    'ETTh1': {\n",
    "        96: {'mse': 0.384, 'mae': 0.402},\n",
    "        192: {'mse': 0.436, 'mae': 0.429},\n",
    "        336: {'mse': 0.491, 'mae': 0.469},\n",
    "        720: {'mse': 0.521, 'mae': 0.491},\n",
    "    },\n",
    "    'ETTm1': {\n",
    "        96: {'mse': 0.334, 'mae': 0.365},\n",
    "        192: {'mse': 0.374, 'mae': 0.385},\n",
    "        336: {'mse': 0.410, 'mae': 0.403},\n",
    "        720: {'mse': 0.478, 'mae': 0.437},\n",
    "    }\n",
    "}\n",
    "\n",
    "# Compare with our results\n",
    "print(\"=\"*70)\n",
    "print(\"COMPARISON WITH PAPER RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for dataset in ['ETTh1', 'ETTm1']:\n",
    "    if dataset in paper_results:\n",
    "        print(f\"\\n{dataset}:\")\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"{'Horizon':>10} {'Paper MSE':>12} {'Our MSE':>12} {'Diff':>10} {'Paper MAE':>12} {'Our MAE':>12} {'Diff':>10}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for horizon in [96, 192, 336, 720]:\n",
    "            if horizon in paper_results[dataset]:\n",
    "                paper = paper_results[dataset][horizon]\n",
    "                our = results_df[(results_df['dataset'] == dataset) & (results_df['pred_len'] == horizon)]\n",
    "                \n",
    "                if not our.empty:\n",
    "                    our_mse = our['test_mse'].values[0]\n",
    "                    our_mae = our['test_mae'].values[0]\n",
    "                    \n",
    "                    mse_diff = ((our_mse - paper['mse']) / paper['mse']) * 100\n",
    "                    mae_diff = ((our_mae - paper['mae']) / paper['mae']) * 100\n",
    "                    \n",
    "                    print(f\"{horizon:>10} {paper['mse']:>12.4f} {our_mse:>12.4f} {mse_diff:>9.2f}% \"\n",
    "                          f\"{paper['mae']:>12.4f} {our_mae:>12.4f} {mae_diff:>9.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results Table (LaTeX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LaTeX table\n",
    "latex_table = []\n",
    "latex_table.append(\"\\\\begin{table}[h]\")\n",
    "latex_table.append(\"\\\\centering\")\n",
    "latex_table.append(\"\\\\caption{TimesNet Results on ETT Datasets}\")\n",
    "latex_table.append(\"\\\\begin{tabular}{lcccc}\")\n",
    "latex_table.append(\"\\\\hline\")\n",
    "latex_table.append(\"Dataset & Horizon & MSE & MAE & RMSE \\\\\\\\\")\n",
    "latex_table.append(\"\\\\hline\")\n",
    "\n",
    "for dataset in DATASETS:\n",
    "    dataset_results = results_df[results_df['dataset'] == dataset].sort_values('pred_len')\n",
    "    for _, row in dataset_results.iterrows():\n",
    "        latex_table.append(f\"{row['dataset']} & {row['pred_len']} & \"\n",
    "                          f\"{row['test_mse']:.4f} & {row['test_mae']:.4f} & {row['test_rmse']:.4f} \\\\\\\\\")\n",
    "    latex_table.append(\"\\\\hline\")\n",
    "\n",
    "latex_table.append(\"\\\\end{tabular}\")\n",
    "latex_table.append(\"\\\\end{table}\")\n",
    "\n",
    "latex_str = \"\\n\".join(latex_table)\n",
    "\n",
    "# Save to file\n",
    "latex_path = f'{RESULTS_DIR}/results_table.tex'\n",
    "with open(latex_path, 'w') as f:\n",
    "    f.write(latex_str)\n",
    "\n",
    "print(f\"✓ LaTeX table saved to {latex_path}\")\n",
    "print(\"\\nLaTeX Table:\")\n",
    "print(latex_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "✅ **Training Complete!**\n",
    "\n",
    "**Trained Models:**\n",
    "- 4 datasets (ETTh1, ETTh2, ETTm1, ETTm2)\n",
    "- 6 prediction horizons each (24, 48, 96, 192, 336, 720)\n",
    "- Total: 24 models\n",
    "\n",
    "**Output Files:**\n",
    "- `results/all_results.csv` - Summary table\n",
    "- `results/all_results_detailed.json` - Detailed results with training curves\n",
    "- `results/error_vs_horizon.png` - Error vs horizon plots\n",
    "- `results/mse_heatmap.png` - MSE heatmap\n",
    "- `results/mae_heatmap.png` - MAE heatmap\n",
    "- `results/results_table.tex` - LaTeX table\n",
    "- `results/{dataset}_{seq_len}_{pred_len}_curves.png` - Individual training curves\n",
    "- `checkpoints/{dataset}_{seq_len}_{pred_len}/checkpoint.pth` - Model checkpoints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
