# TimesNet Architecture Explained

## ğŸ¯ Core Understanding

### What TimesNet IS:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ENCODER-ONLY MODEL                    â”‚
â”‚                                                          â”‚
â”‚  Input â†’ Embedding â†’ TimesBlocks â†’ Linear â†’ Predictions â”‚
â”‚                         (FFT)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
---

## ğŸ“Š Data Flow Diagram

```
INPUT DATA (CSV)
    â”‚
    â”œâ”€â”€â”€ date: 2016-07-01 00:00:00
    â”œâ”€â”€â”€ HUFL: 5.827
    â”œâ”€â”€â”€ HULL: 2.009
    â”œâ”€â”€â”€ MUFL: 1.599
    â”œâ”€â”€â”€ MULL: 0.462
    â”œâ”€â”€â”€ LUFL: 4.203
    â”œâ”€â”€â”€ LULL: 1.340
    â””â”€â”€â”€ OT: 30.531

         â†“

DATASET PROCESSING (dataset.py)
    â”‚
    â”œâ”€â”€â”€ StandardScaler (if scale=True)
    â”‚    â”œâ”€â”€â”€ Fit on train data only
    â”‚    â””â”€â”€â”€ Transform all data
    â”‚
    â”œâ”€â”€â”€ Time Features (timeenc)
    â”‚    â”œâ”€â”€â”€ Manual (0): [month, day, weekday, hour]
    â”‚    â””â”€â”€â”€ Fourier (1): [normalized temporal values]
    â”‚
    â””â”€â”€â”€ Train/Val/Test Split
         â”œâ”€â”€â”€ Train: 70%
         â”œâ”€â”€â”€ Val: 10%
         â””â”€â”€â”€ Test: 20%

         â†“

DATASET __getitem__ RETURNS
    â”‚
    â”œâ”€â”€â”€ seq_x: [seq_len=96, features=7]       # Input sequence
    â”œâ”€â”€â”€ seq_y: [pred_len=96, features=7]      # Target (future values)
    â””â”€â”€â”€ seq_x_mark: [seq_len=96, time_feat=4] # Time features

    âŒ NO seq_y_mark (removed!)

         â†“

DATALOADER (batch processing)
    â”‚
    â””â”€â”€â”€ Batches: [batch_size=32, seq_len=96, features=7]

         â†“

MODEL FORWARD PASS (model.py)
    â”‚
    â”œâ”€â”€â”€ Input: batch_x [B, 96, 7], batch_x_mark [B, 96, 4]
    â”‚
    â”œâ”€â”€â”€ 1. NORMALIZATION (Non-stationary)
    â”‚    â”œâ”€â”€â”€ means = x.mean(dim=1)
    â”‚    â”œâ”€â”€â”€ stdev = x.std(dim=1)
    â”‚    â””â”€â”€â”€ x_normalized = (x - means) / stdev
    â”‚
    â”œâ”€â”€â”€ 2. EMBEDDING
    â”‚    â”œâ”€â”€â”€ Value Embedding (1D Conv): [B, 96, 7] â†’ [B, 96, 16]
    â”‚    â”œâ”€â”€â”€ Temporal Embedding: [B, 96, 4] â†’ [B, 96, 16]
    â”‚    â””â”€â”€â”€ Positional Embedding: [B, 96, 16]
    â”‚    â””â”€â”€â”€ Combined: [B, 96, 16]
    â”‚
    â”œâ”€â”€â”€ 3. LINEAR PROJECTION
    â”‚    â””â”€â”€â”€ [B, 96, 16] â†’ [B, 144, 16]  (96+96=144)
    â”‚
    â”œâ”€â”€â”€ 4. TIMESBLOCKS (Ã—2)
    â”‚    â”‚
    â”‚    â”œâ”€â”€â”€ A. FFT_for_Period (find top-k=5 frequencies)
    â”‚    â”‚    â”œâ”€â”€â”€ FFT: [B, 144, 16] â†’ frequency domain
    â”‚    â”‚    â”œâ”€â”€â”€ Amplitude: |FFT|.mean(0).mean(-1)
    â”‚    â”‚    â””â”€â”€â”€ Top-k: argmax(amplitudes, k=5) â†’ periods
    â”‚    â”‚
    â”‚    â”œâ”€â”€â”€ B. Reshape to 2D
    â”‚    â”‚    â””â”€â”€â”€ [B, 144, 16] â†’ [B, 16, period, 144/period]
    â”‚    â”‚
    â”‚    â”œâ”€â”€â”€ C. Inception_Block_V1 (multi-scale 2D Conv)
    â”‚    â”‚    â”œâ”€â”€â”€ Kernel 1Ã—1: [B, 16, *, *] â†’ [B, 32, *, *]
    â”‚    â”‚    â”œâ”€â”€â”€ Kernel 3Ã—3: [B, 16, *, *] â†’ [B, 32, *, *]
    â”‚    â”‚    â”œâ”€â”€â”€ Kernel 5Ã—5: [B, 16, *, *] â†’ [B, 32, *, *]
    â”‚    â”‚    â”œâ”€â”€â”€ ... (6 kernels total)
    â”‚    â”‚    â””â”€â”€â”€ Average: [B, 32, *, *]
    â”‚    â”‚
    â”‚    â”œâ”€â”€â”€ D. GELU activation
    â”‚    â”‚
    â”‚    â”œâ”€â”€â”€ E. Second Inception Block
    â”‚    â”‚    â””â”€â”€â”€ [B, 32, *, *] â†’ [B, 16, *, *]
    â”‚    â”‚
    â”‚    â”œâ”€â”€â”€ F. Reshape back to 1D
    â”‚    â”‚    â””â”€â”€â”€ [B, 16, *, *] â†’ [B, 144, 16]
    â”‚    â”‚
    â”‚    â”œâ”€â”€â”€ G. Weight by amplitudes (adaptive aggregation)
    â”‚    â”‚    â””â”€â”€â”€ softmax(amplitudes) * outputs
    â”‚    â”‚
    â”‚    â””â”€â”€â”€ H. Residual connection
    â”‚         â””â”€â”€â”€ output = weighted_sum + input
    â”‚
    â”œâ”€â”€â”€ 5. LAYER NORM
    â”‚    â””â”€â”€â”€ [B, 144, 16] â†’ normalized
    â”‚
    â”œâ”€â”€â”€ 6. PROJECTION
    â”‚    â””â”€â”€â”€ [B, 144, 16] â†’ [B, 144, 7]
    â”‚
    â”œâ”€â”€â”€ 7. DE-NORMALIZATION
    â”‚    â””â”€â”€â”€ output * stdev + means
    â”‚
    â””â”€â”€â”€ 8. EXTRACT PREDICTIONS
         â””â”€â”€â”€ [B, 144, 7][-96:] â†’ [B, 96, 7]

         â†“

OUTPUT
    â””â”€â”€â”€ Predictions: [batch_size=32, pred_len=96, features=7]
```

---

## ğŸ”§ Two Different FFT Uses

### 1. Time Encoding (Optional, in dataset.py)
```python
if timeenc == 1:  # Fourier time features
    # Encodes calendar information as continuous values
    hour / 23.0 - 0.5
    dayofweek / 6.0 - 0.5
    # etc.
```
**Purpose**: Tell model "what time it is"
**Used**: Optionally for time features
**Paper uses**: `timeenc=0` (manual encoding)

### 2. Period Detection (Always, in model.py)
```python
def FFT_for_Period(x, k=5):  # TimesNet core!
    xf = torch.fft.rfft(x, dim=1)
    amplitudes = abs(xf).mean(0).mean(-1)
    top_frequencies = topk(amplitudes, k)
    periods = seq_len // top_frequencies
```
**Purpose**: Find dominant periods (daily, weekly, etc.)
**Used**: Always, with top_k=5
**This is TimesNet's innovation!**

---

## ğŸ“ˆ Example: ETTh1 96â†’96 Prediction

### Input:
```
Sequence: [2016-07-01 00:00, ..., 2016-07-04 23:00]  # 96 hours
Features: [HUFL, HULL, MUFL, MULL, LUFL, LULL, OT]   # 7 features
```

### Processing:
1. **Normalize**: zero mean, unit variance
2. **Embed**: 7 features â†’ 16 dimensions
3. **Extend**: 96 timesteps â†’ 144 timesteps (96+96)
4. **FFT**: Find top-5 periods (e.g., 24h, 12h, 8h, 6h, 4h)
5. **2D Conv**: Multi-scale pattern extraction for each period
6. **Aggregate**: Weighted sum based on period importance
7. **Project**: 16 dimensions â†’ 7 features
8. **Extract**: Last 96 timesteps

### Output:
```
Predictions: [2016-07-05 00:00, ..., 2016-07-08 23:00]  # 96 hours
Features: [HUFL, HULL, MUFL, MULL, LUFL, LULL, OT]      # 7 features
```

---

## ğŸ”‘ Key Design Choices

### 1. **Why no decoder?**
- TimesNet uses direct projection: seq_len â†’ seq_len+pred_len
- Then extracts last pred_len steps
- Simpler and more efficient than autoregressive decoding

### 2. **Why 2D convolutions?**
- Reshape 1D time series based on discovered periods
- Vertical axis = period, Horizontal axis = cycles
- 2D patterns capture both intra-period and inter-period variations

### 3. **Why multiple periods (top-k)?**
- Real data has multiple periodicities (hourly, daily, weekly)
- Different periods capture different patterns
- Adaptive aggregation weights them by importance

### 4. **Why Inception blocks?**
- Multi-scale kernels (1Ã—1, 3Ã—3, 5Ã—5, ..., 11Ã—11)
- Capture both fine and coarse patterns
- Parameter-efficient (shared input)

---

## ğŸ“ Paper Values Reference

### ETTh1 (96 â†’ 96)
```python
# Data
seq_len = 96       # 4 days of hourly data
pred_len = 96      # Predict next 4 days
scale = True       # Normalize
timeenc = 0        # Manual time encoding

# Model
enc_in = 7         # 7 features
c_out = 7
d_model = 16       # Small for ETT
d_ff = 32
top_k = 5          # Top-5 periods
e_layers = 2       # 2 TimesBlocks
num_kernels = 6    # 6 inception kernels
dropout = 0.1
embed = 'fixed'    # Fixed embeddings

# Training
lr = 0.0001        # Adam
batch_size = 32
epochs = 10
patience = 3       # Early stopping
```

### Other Horizons (ETTh1)
```python
96 â†’ 192:  same params, just pred_len=192
96 â†’ 336:  same params, just pred_len=336
96 â†’ 720:  same params, just pred_len=720
```

---

## âœ… Implementation Checklist

- [x] Dataset returns 3 items (x, y, x_mark)
- [x] Model takes 2 arguments (x_enc, x_mark_enc)
- [x] No decoder inputs or label_len
- [x] FFT finds top-k periods in model
- [x] 2D convolutions for multi-period modeling
- [x] Adaptive aggregation by amplitude
- [x] Non-stationary normalization
- [x] Paper hyperparameters (d_model=16, top_k=5, etc.)

---

## ğŸš€ That's TimesNet!

**Core Innovation**: Transform 1D time series to 2D based on discovered periods, then use 2D convolutions to capture complex temporal patterns.

**Key Advantage**: No need for:
- âŒ Complex attention mechanisms
- âŒ Decoder architectures
- âŒ Autoregressive generation
- âŒ Position-dependent operations

**Result**: Simple, efficient, and effective! ğŸ‰
