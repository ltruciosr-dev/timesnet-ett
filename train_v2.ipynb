{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GhgW_MGL4V6f"
   },
   "source": "# TimesNet Training V2: Hyperparameter Experiments\n\nThis notebook explores different hyperparameter configurations to improve model performance.\n\n**Focus Areas:**\n1. **Learning Rate Schedule** - Test constant vs decaying LR\n2. **Training Duration** - More epochs to allow convergence\n3. **Early Stopping** - Adjust patience\n4. **Model Capacity** - Test larger d_model/d_ff\n\n**Baseline Issues Identified:**\n- `lradj='type1'` halves LR every epoch (too aggressive!)\n- Only 10 epochs might be insufficient\n- Patience of 3 might stop too early\n\n**Experiments:**\n- V1: Baseline (current config)\n- V2: Constant LR + more epochs\n- V3: Larger model capacity\n- V4: Best combination"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GmeZngpl4V6h"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!git clone https://github.com/ltruciosr-dev/timesnet-ett\n",
    "\n",
    "# Change to the cloned repository directory\n",
    "%cd timesnet-ett"
   ],
   "metadata": {
    "id": "AnPkHqZA4a6k",
    "outputId": "caa680b5-fb75-42cc-c36a-89b1f4b018f4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cloning into 'timesnet-ett'...\n",
      "remote: Enumerating objects: 15, done.\u001b[K\n",
      "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
      "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
      "remote: Total 15 (delta 0), reused 15 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (15/15), 2.92 MiB | 6.22 MiB/s, done.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9OJZyIkK4V6h",
    "outputId": "a08d0bae-8fd0-425a-cc54-5cf8bc004365",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": "import sys\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nimport json\n\n# Add src to path\nsys.path.append('./src')\n\nfrom src.train import TimesNetTrainer\nfrom src.evaluate import (\n    evaluate_and_save_results,\n    plot_training_curves,\n    visualize_horizon_predictions,\n    plot_comprehensive_training_summary\n)\n\n# Set style\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n\nprint(\"âœ“ Imports successful\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pClMt56L4V6h"
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fgSuegmr4V6h",
    "outputId": "5453675e-1e90-468b-c32d-4ed09f3afbbb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": "# Base paths\nROOT_PATH = './ETDataset/ETT-small/'\nCHECKPOINT_BASE = './checkpoints_v2'\nRESULTS_DIR = './results_v2'\n\n# Create directories\nos.makedirs(CHECKPOINT_BASE, exist_ok=True)\nos.makedirs(RESULTS_DIR, exist_ok=True)\n\n# Datasets (test on ETTh1 first, then expand)\nDATASETS = ['ETTh1']\n\n# Fixed input length\nSEQ_LEN = 96\n\n# Test on fewer horizons initially\nPRED_LENS = [96, 192]  # Start with these, expand if successful\n\n# ========== HYPERPARAMETER EXPERIMENTS ==========\n\n# V1: Baseline (from original paper)\nEXPERIMENT_CONFIGS = {\n    'V1_baseline': {\n        'name': 'Baseline (Paper Config)',\n        'description': 'Original config - aggressive LR decay',\n        'model_config': {'d_model': 16, 'd_ff': 32},\n        'train_config': {\n            'enc_in': 7,\n            'c_out': 7,\n            'top_k': 5,\n            'e_layers': 2,\n            'num_kernels': 6,\n            'dropout': 0.1,\n            'embed': 'fixed',\n            'batch_size': 32,\n            'learning_rate': 0.0001,\n            'train_epochs': 10,\n            'patience': 3,\n            'lradj': 'type1',  # Halves LR every epoch\n            'use_amp': False,\n            'num_workers': 0,\n        }\n    },\n    \n    'V2_constant_lr': {\n        'name': 'Constant LR + More Epochs',\n        'description': 'Fixed LR, 30 epochs, patience=5',\n        'model_config': {'d_model': 16, 'd_ff': 32},\n        'train_config': {\n            'enc_in': 7,\n            'c_out': 7,\n            'top_k': 5,\n            'e_layers': 2,\n            'num_kernels': 6,\n            'dropout': 0.1,\n            'embed': 'fixed',\n            'batch_size': 32,\n            'learning_rate': 0.0001,\n            'train_epochs': 30,\n            'patience': 5,\n            'lradj': 'constant',  # No LR decay\n            'use_amp': False,\n            'num_workers': 0,\n        }\n    },\n    \n    'V3_larger_model': {\n        'name': 'Larger Model Capacity',\n        'description': 'd_model=32, d_ff=64, constant LR',\n        'model_config': {'d_model': 32, 'd_ff': 64},  # 2x capacity\n        'train_config': {\n            'enc_in': 7,\n            'c_out': 7,\n            'top_k': 5,\n            'e_layers': 2,\n            'num_kernels': 6,\n            'dropout': 0.1,\n            'embed': 'fixed',\n            'batch_size': 32,\n            'learning_rate': 0.0001,\n            'train_epochs': 30,\n            'patience': 5,\n            'lradj': 'constant',\n            'use_amp': False,\n            'num_workers': 0,\n        }\n    },\n    \n    'V4_best_combo': {\n        'name': 'Best Combination',\n        'description': 'Larger model + lower LR + more epochs',\n        'model_config': {'d_model': 32, 'd_ff': 64},\n        'train_config': {\n            'enc_in': 7,\n            'c_out': 7,\n            'top_k': 5,\n            'e_layers': 2,\n            'num_kernels': 6,\n            'dropout': 0.15,  # Slightly more dropout\n            'embed': 'fixed',\n            'batch_size': 32,\n            'learning_rate': 0.00005,  # Lower LR\n            'train_epochs': 50,\n            'patience': 7,\n            'lradj': 'constant',\n            'use_amp': False,\n            'num_workers': 0,\n        }\n    },\n}\n\nprint(f\"âœ“ Hyperparameter Experiments Configured\")\nprint(f\"  - Datasets: {DATASETS}\")\nprint(f\"  - Input length: {SEQ_LEN}\")\nprint(f\"  - Prediction horizons: {PRED_LENS}\")\nprint(f\"  - Experiments: {list(EXPERIMENT_CONFIGS.keys())}\")\nprint(f\"  - Total runs: {len(DATASETS)} Ã— {len(PRED_LENS)} Ã— {len(EXPERIMENT_CONFIGS)} = {len(DATASETS) * len(PRED_LENS) * len(EXPERIMENT_CONFIGS)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otYKSKVY4V6i"
   },
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nYcfecNm4V6i"
   },
   "outputs": [],
   "source": "def train_single_experiment(dataset_name, pred_len, experiment_id, experiment_config):\n    \"\"\"\n    Train a single TimesNet experiment with specific hyperparameters\n\n    Args:\n        dataset_name: Dataset name (e.g., 'ETTh1')\n        pred_len: Prediction horizon\n        experiment_id: Experiment identifier (e.g., 'V2_constant_lr')\n        experiment_config: Experiment configuration dict\n\n    Returns:\n        dict: Training results\n    \"\"\"\n    print(\"\\n\" + \"=\"*70)\n    print(f\"Experiment: {experiment_config['name']}\")\n    print(f\"Dataset: {dataset_name} | seq_len={SEQ_LEN} â†’ pred_len={pred_len}\")\n    print(f\"Description: {experiment_config['description']}\")\n    print(\"=\"*70)\n\n    # Create config\n    config = {\n        'root_path': ROOT_PATH,\n        'data_path': f'{dataset_name}.csv',\n        'seq_len': SEQ_LEN,\n        'pred_len': pred_len,\n        'checkpoints': f'{CHECKPOINT_BASE}/{experiment_id}/{dataset_name}_{SEQ_LEN}_{pred_len}',\n        **experiment_config['train_config'],\n        **experiment_config['model_config']\n    }\n\n    # Create checkpoint directory\n    os.makedirs(config['checkpoints'], exist_ok=True)\n\n    # Train\n    trainer = TimesNetTrainer(config)\n    train_losses, val_losses = trainer.train()\n\n    # Test\n    test_results = trainer.test()\n\n    # ========== ENHANCED VISUALIZATIONS ==========\n    \n    print(\"\\nðŸ“Š Creating visualizations...\")\n    \n    # Create experiment-specific subdirectory\n    exp_results_dir = f'{RESULTS_DIR}/{experiment_id}'\n    os.makedirs(exp_results_dir, exist_ok=True)\n    \n    # 1. Horizon predictions\n    print(\"  â†’ Horizon predictions...\")\n    visualize_horizon_predictions(\n        model=trainer.model,\n        train_loader=trainer.train_loader,\n        val_loader=trainer.val_loader,\n        test_loader=trainer.test_loader,\n        device=trainer.device,\n        scaler=trainer.train_dataset.scaler,\n        seq_len=config['seq_len'],\n        pred_len=config['pred_len'],\n        num_samples=3,\n        feature_idx=0,\n        feature_name='OT',\n        save_path=f'{exp_results_dir}/{dataset_name}_{SEQ_LEN}_{pred_len}_horizons.png'\n    )\n    \n    # 2. Comprehensive training summary\n    print(\"  â†’ Comprehensive summary...\")\n    plot_comprehensive_training_summary(\n        train_losses=train_losses,\n        val_losses=val_losses,\n        test_results=test_results,\n        dataset_name=f\"{dataset_name} ({experiment_config['name']})\",\n        seq_len=config['seq_len'],\n        pred_len=config['pred_len'],\n        save_path=f'{exp_results_dir}/{dataset_name}_{SEQ_LEN}_{pred_len}_summary.png'\n    )\n    \n    # 3. Evaluate on all splits\n    print(\"  â†’ Evaluating all splits...\")\n    all_split_results = evaluate_and_save_results(\n        model=trainer.model,\n        train_loader=trainer.train_loader,\n        val_loader=trainer.val_loader,\n        test_loader=trainer.test_loader,\n        device=trainer.device,\n        scaler=trainer.train_dataset.scaler,\n        save_path=f'{exp_results_dir}/{dataset_name}_{SEQ_LEN}_{pred_len}_all_metrics.txt'\n    )\n\n    # Prepare results\n    results = {\n        'experiment_id': experiment_id,\n        'experiment_name': experiment_config['name'],\n        'dataset': dataset_name,\n        'seq_len': SEQ_LEN,\n        'pred_len': pred_len,\n        'd_model': config['d_model'],\n        'd_ff': config['d_ff'],\n        'learning_rate': config['learning_rate'],\n        'lradj': config['lradj'],\n        'train_epochs': config['train_epochs'],\n        'patience': config['patience'],\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'test_mse': test_results['mse'],\n        'test_mae': test_results['mae'],\n        'test_rmse': test_results['rmse'],\n        'final_epoch': len(train_losses),\n        'train_mse': all_split_results['train']['mse'],\n        'train_mae': all_split_results['train']['mae'],\n        'val_mse': all_split_results['val']['mse'],\n        'val_mae': all_split_results['val']['mae'],\n    }\n\n    print(f\"\\nâœ“ Completed: {experiment_id} - {dataset_name}_{SEQ_LEN}_{pred_len}\")\n    print(f\"  - Train MSE: {all_split_results['train']['mse']:.6f}\")\n    print(f\"  - Val MSE:   {all_split_results['val']['mse']:.6f}\")\n    print(f\"  - Test MSE:  {test_results['mse']:.6f}\")\n    print(f\"  - Test MAE:  {test_results['mae']:.6f}\")\n\n    return results"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAIUi-py4V6i"
   },
   "source": [
    "## Train All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KEVL1a294V6i",
    "outputId": "35784038-c9f4-40ea-a979-8afda8ba1c32",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": "# Store all results\nall_results = []\n\n# Run all experiments\ntotal_experiments = len(DATASETS) * len(PRED_LENS) * len(EXPERIMENT_CONFIGS)\ncurrent_experiment = 0\n\nfor dataset_name in DATASETS:\n    for pred_len in PRED_LENS:\n        for experiment_id, experiment_config in EXPERIMENT_CONFIGS.items():\n            current_experiment += 1\n\n            print(f\"\\n{'#'*70}\")\n            print(f\"# Run {current_experiment}/{total_experiments}\")\n            print(f\"{'#'*70}\")\n\n            try:\n                results = train_single_experiment(\n                    dataset_name=dataset_name,\n                    pred_len=pred_len,\n                    experiment_id=experiment_id,\n                    experiment_config=experiment_config\n                )\n                all_results.append(results)\n\n            except Exception as e:\n                print(f\"\\nâŒ Error in {experiment_id} - {dataset_name}_{SEQ_LEN}_{pred_len}: {e}\")\n                import traceback\n                traceback.print_exc()\n                continue\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"âœ“ ALL EXPERIMENTS COMPLETED!\")\nprint(\"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "koruQ8qf4V6i"
   },
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oo0xaPN54V6i"
   },
   "outputs": [],
   "source": "# Convert to DataFrame for easy analysis\nresults_df = pd.DataFrame([{\n    'dataset': r['dataset'],\n    'seq_len': r['seq_len'],\n    'pred_len': r['pred_len'],\n    'd_model': r['d_model'],\n    'd_ff': r['d_ff'],\n    'train_mse': r['train_mse'],\n    'train_mae': r['train_mae'],\n    'val_mse': r['val_mse'],\n    'val_mae': r['val_mae'],\n    'test_mse': r['test_mse'],\n    'test_mae': r['test_mae'],\n    'test_rmse': r['test_rmse'],\n    'final_epoch': r['final_epoch']\n} for r in all_results])\n\n# Save to CSV\ncsv_path = f'{RESULTS_DIR}/all_results.csv'\nresults_df.to_csv(csv_path, index=False)\nprint(f\"âœ“ Results saved to {csv_path}\")\n\n# Save detailed results (with training curves) to JSON\njson_path = f'{RESULTS_DIR}/all_results_detailed.json'\nwith open(json_path, 'w') as f:\n    json.dump(all_results, f, indent=2)\nprint(f\"âœ“ Detailed results saved to {json_path}\")\n\n# Display summary\nprint(\"\\n\" + \"=\"*70)\nprint(\"RESULTS SUMMARY\")\nprint(\"=\"*70)\ndisplay(results_df)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Is0ZJ9C4V6i"
   },
   "source": "# Convert to DataFrame for analysis\nresults_df = pd.DataFrame([{\n    'experiment_id': r['experiment_id'],\n    'experiment_name': r['experiment_name'],\n    'dataset': r['dataset'],\n    'pred_len': r['pred_len'],\n    'd_model': r['d_model'],\n    'd_ff': r['d_ff'],\n    'learning_rate': r['learning_rate'],\n    'lradj': r['lradj'],\n    'train_epochs': r['train_epochs'],\n    'final_epoch': r['final_epoch'],\n    'train_mse': r['train_mse'],\n    'val_mse': r['val_mse'],\n    'test_mse': r['test_mse'],\n    'test_mae': r['test_mae'],\n    'test_rmse': r['test_rmse'],\n} for r in all_results])\n\n# Save to CSV\ncsv_path = f'{RESULTS_DIR}/hyperparameter_experiments.csv'\nresults_df.to_csv(csv_path, index=False)\nprint(f\"âœ“ Results saved to {csv_path}\")\n\n# Save detailed results to JSON\njson_path = f'{RESULTS_DIR}/hyperparameter_experiments_detailed.json'\nwith open(json_path, 'w') as f:\n    json.dump(all_results, f, indent=2)\nprint(f\"âœ“ Detailed results saved to {json_path}\")\n\n# Display summary\nprint(\"\\n\" + \"=\"*70)\nprint(\"HYPERPARAMETER EXPERIMENT RESULTS\")\nprint(\"=\"*70)\ndisplay(results_df)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auev4rqz4V6i"
   },
   "source": [
    "### Results by Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5VT_jCca4V6i"
   },
   "outputs": [],
   "source": "# Group by dataset\nfor dataset in DATASETS:\n    print(f\"\\n{'='*60}\")\n    print(f\"{dataset} Results\")\n    print(f\"{'='*60}\")\n\n    dataset_results = results_df[results_df['dataset'] == dataset]\n    display(dataset_results[['pred_len', 'train_mse', 'val_mse', 'test_mse', 'test_mae', 'test_rmse', 'final_epoch']])"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "riQOi-Nf4V6i"
   },
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XMdk3L-e4V6i"
   },
   "outputs": [],
   "source": [
    "# Plot MSE vs Prediction Horizon for each dataset\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, dataset in enumerate(DATASETS):\n",
    "    ax = axes[idx]\n",
    "    dataset_results = results_df[results_df['dataset'] == dataset].sort_values('pred_len')\n",
    "\n",
    "    ax.plot(dataset_results['pred_len'], dataset_results['test_mse'],\n",
    "            marker='o', linewidth=2, markersize=8, label='MSE')\n",
    "    ax.plot(dataset_results['pred_len'], dataset_results['test_mae'],\n",
    "            marker='s', linewidth=2, markersize=8, label='MAE')\n",
    "\n",
    "    ax.set_xlabel('Prediction Horizon', fontsize=12)\n",
    "    ax.set_ylabel('Error', fontsize=12)\n",
    "    ax.set_title(f'{dataset} - Error vs Prediction Horizon', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_DIR}/error_vs_horizon.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ“ Plot saved to {RESULTS_DIR}/error_vs_horizon.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ESSrnHRd4V6i"
   },
   "outputs": [],
   "source": "# ========== EXPERIMENT COMPARISON ==========\n\n# Compare experiments for each horizon\nfig, axes = plt.subplots(1, len(PRED_LENS), figsize=(7*len(PRED_LENS), 6))\nif len(PRED_LENS) == 1:\n    axes = [axes]\n\nfor idx, pred_len in enumerate(PRED_LENS):\n    ax = axes[idx]\n    \n    horizon_results = results_df[results_df['pred_len'] == pred_len].sort_values('experiment_id')\n    \n    x = np.arange(len(horizon_results))\n    width = 0.25\n    \n    ax.bar(x - width, horizon_results['train_mse'], width, label='Train MSE', alpha=0.8)\n    ax.bar(x, horizon_results['val_mse'], width, label='Val MSE', alpha=0.8)\n    ax.bar(x + width, horizon_results['test_mse'], width, label='Test MSE', alpha=0.8)\n    \n    ax.set_xlabel('Experiment', fontsize=12)\n    ax.set_ylabel('MSE', fontsize=12)\n    ax.set_title(f'Prediction Horizon {pred_len}', fontsize=14, fontweight='bold')\n    ax.set_xticks(x)\n    ax.set_xticklabels(horizon_results['experiment_id'], rotation=45, ha='right')\n    ax.legend(fontsize=10)\n    ax.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.savefig(f'{RESULTS_DIR}/experiment_comparison.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(f\"âœ“ Comparison plot saved to {RESULTS_DIR}/experiment_comparison.png\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YGMa7-gv4V6i"
   },
   "outputs": [],
   "source": [
    "# Heatmap of MAE across datasets and horizons\n",
    "pivot_mae = results_df.pivot(index='dataset', columns='pred_len', values='test_mae')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(pivot_mae, annot=True, fmt='.4f', cmap='YlGnBu',\n",
    "            cbar_kws={'label': 'Test MAE'}, linewidths=0.5)\n",
    "plt.title('Test MAE: Dataset vs Prediction Horizon', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Prediction Horizon', fontsize=12)\n",
    "plt.ylabel('Dataset', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_DIR}/mae_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ“ Heatmap saved to {RESULTS_DIR}/mae_heatmap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9mAUm7M4V6i"
   },
   "source": [
    "### Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lyeADhfL4V6i"
   },
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\"*70)\n",
    "print(\"STATISTICAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nOverall Statistics:\")\n",
    "print(results_df[['test_mse', 'test_mae', 'test_rmse']].describe())\n",
    "\n",
    "print(\"\\nBest Results (by MSE):\")\n",
    "best_mse = results_df.loc[results_df.groupby('dataset')['test_mse'].idxmin()]\n",
    "display(best_mse[['dataset', 'pred_len', 'test_mse', 'test_mae', 'test_rmse']])\n",
    "\n",
    "print(\"\\nBest Results (by MAE):\")\n",
    "best_mae = results_df.loc[results_df.groupby('dataset')['test_mae'].idxmin()]\n",
    "display(best_mae[['dataset', 'pred_len', 'test_mse', 'test_mae', 'test_rmse']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5fxSjSR4V6i"
   },
   "source": "# ========== EXPERIMENT ANALYSIS ==========\n\nprint(\"=\"*70)\nprint(\"EXPERIMENT PERFORMANCE RANKING\")\nprint(\"=\"*70)\n\n# Group by horizon and rank by test MSE\nfor pred_len in PRED_LENS:\n    print(f\"\\nðŸ“Š Horizon {pred_len}:\")\n    print(\"-\" * 60)\n    horizon_df = results_df[results_df['pred_len'] == pred_len].sort_values('test_mse')\n    \n    for idx, row in horizon_df.iterrows():\n        print(f\"  {row['experiment_id']:20s} | Test MSE: {row['test_mse']:.6f} | \"\n              f\"Val MSE: {row['val_mse']:.6f} | Epochs: {row['final_epoch']:2d}/{row['train_epochs']:2d}\")\n    \n    best = horizon_df.iloc[0]\n    print(f\"\\n  âœ… BEST: {best['experiment_id']} - {best['experiment_name']}\")\n    print(f\"     Test MSE: {best['test_mse']:.6f}\")\n    print(f\"     Test MAE: {best['test_mae']:.6f}\")\n\n# Overall best configuration\nprint(\"\\n\" + \"=\"*70)\nprint(\"OVERALL ANALYSIS\")\nprint(\"n=\"*70)\n\n# Average performance across horizons\navg_performance = results_df.groupby('experiment_id').agg({\n    'test_mse': 'mean',\n    'test_mae': 'mean',\n    'val_mse': 'mean',\n    'final_epoch': 'mean'\n}).sort_values('test_mse')\n\nprint(\"\\nðŸ“ˆ Average Performance Across All Horizons:\")\nprint(avg_performance)\n\nbest_overall = avg_performance.index[0]\nprint(f\"\\nâœ… BEST OVERALL CONFIGURATION: {best_overall}\")\nprint(f\"   Average Test MSE: {avg_performance.loc[best_overall, 'test_mse']:.6f}\")\nprint(f\"   Average Test MAE: {avg_performance.loc[best_overall, 'test_mae']:.6f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7u0NLdwv4V6i"
   },
   "outputs": [],
   "source": [
    "# Paper results (from TimesNet paper Table 1)\n",
    "paper_results = {\n",
    "    'ETTh1': {\n",
    "        96: {'mse': 0.384, 'mae': 0.402},\n",
    "        192: {'mse': 0.436, 'mae': 0.429},\n",
    "        336: {'mse': 0.491, 'mae': 0.469},\n",
    "        720: {'mse': 0.521, 'mae': 0.491},\n",
    "    },\n",
    "    'ETTm1': {\n",
    "        96: {'mse': 0.334, 'mae': 0.365},\n",
    "        192: {'mse': 0.374, 'mae': 0.385},\n",
    "        336: {'mse': 0.410, 'mae': 0.403},\n",
    "        720: {'mse': 0.478, 'mae': 0.437},\n",
    "    }\n",
    "}\n",
    "\n",
    "# Compare with our results\n",
    "print(\"=\"*70)\n",
    "print(\"COMPARISON WITH PAPER RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for dataset in ['ETTh1', 'ETTm1']:\n",
    "    if dataset in paper_results:\n",
    "        print(f\"\\n{dataset}:\")\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"{'Horizon':>10} {'Paper MSE':>12} {'Our MSE':>12} {'Diff':>10} {'Paper MAE':>12} {'Our MAE':>12} {'Diff':>10}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        for horizon in [96, 192, 336, 720]:\n",
    "            if horizon in paper_results[dataset]:\n",
    "                paper = paper_results[dataset][horizon]\n",
    "                our = results_df[(results_df['dataset'] == dataset) & (results_df['pred_len'] == horizon)]\n",
    "\n",
    "                if not our.empty:\n",
    "                    our_mse = our['test_mse'].values[0]\n",
    "                    our_mae = our['test_mae'].values[0]\n",
    "\n",
    "                    mse_diff = ((our_mse - paper['mse']) / paper['mse']) * 100\n",
    "                    mae_diff = ((our_mae - paper['mae']) / paper['mae']) * 100\n",
    "\n",
    "                    print(f\"{horizon:>10} {paper['mse']:>12.4f} {our_mse:>12.4f} {mse_diff:>9.2f}% \"\n",
    "                          f\"{paper['mae']:>12.4f} {our_mae:>12.4f} {mae_diff:>9.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fH3-zI64V6i"
   },
   "source": [
    "## Export Results Table (LaTeX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ClTJaIi4V6i"
   },
   "outputs": [],
   "source": [
    "# Create LaTeX table\n",
    "latex_table = []\n",
    "latex_table.append(\"\\\\begin{table}[h]\")\n",
    "latex_table.append(\"\\\\centering\")\n",
    "latex_table.append(\"\\\\caption{TimesNet Results on ETT Datasets}\")\n",
    "latex_table.append(\"\\\\begin{tabular}{lcccc}\")\n",
    "latex_table.append(\"\\\\hline\")\n",
    "latex_table.append(\"Dataset & Horizon & MSE & MAE & RMSE \\\\\\\\\")\n",
    "latex_table.append(\"\\\\hline\")\n",
    "\n",
    "for dataset in DATASETS:\n",
    "    dataset_results = results_df[results_df['dataset'] == dataset].sort_values('pred_len')\n",
    "    for _, row in dataset_results.iterrows():\n",
    "        latex_table.append(f\"{row['dataset']} & {row['pred_len']} & \"\n",
    "                          f\"{row['test_mse']:.4f} & {row['test_mae']:.4f} & {row['test_rmse']:.4f} \\\\\\\\\")\n",
    "    latex_table.append(\"\\\\hline\")\n",
    "\n",
    "latex_table.append(\"\\\\end{tabular}\")\n",
    "latex_table.append(\"\\\\end{table}\")\n",
    "\n",
    "latex_str = \"\\n\".join(latex_table)\n",
    "\n",
    "# Save to file\n",
    "latex_path = f'{RESULTS_DIR}/results_table.tex'\n",
    "with open(latex_path, 'w') as f:\n",
    "    f.write(latex_str)\n",
    "\n",
    "print(f\"âœ“ LaTeX table saved to {latex_path}\")\n",
    "print(\"\\nLaTeX Table:\")\n",
    "print(latex_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlnSEBFr4V6i"
   },
   "source": "## Summary\n\nâœ… **Hyperparameter Experiments Complete!**\n\n**Experiments Run:**\n- V1: Baseline (Paper Config - aggressive LR decay)\n- V2: Constant LR + More Epochs\n- V3: Larger Model Capacity\n- V4: Best Combination\n\n**Output Files:**\n\nðŸ“Š **Results Data:**\n- `results_v2/hyperparameter_experiments.csv` - Full comparison table\n- `results_v2/hyperparameter_experiments_detailed.json` - Detailed results\n\nðŸ“ˆ **Visualizations per experiment (4 experiments Ã— 2 horizons Ã— 3 plots = 24 files):**\n- `results_v2/{experiment_id}/{dataset}_{seq_len}_{pred_len}_horizons.png`\n- `results_v2/{experiment_id}/{dataset}_{seq_len}_{pred_len}_summary.png`\n- `results_v2/{experiment_id}/{dataset}_{seq_len}_{pred_len}_all_metrics.txt`\n\nðŸ“Š **Comparison Plots:**\n- `results_v2/experiment_comparison.png` - Side-by-side experiment comparison\n\nðŸ’¾ **Model Checkpoints:**\n- `checkpoints_v2/{experiment_id}/{dataset}_{seq_len}_{pred_len}/checkpoint.pth`\n\n**Key Insights to Look For:**\n1. Does constant LR improve over aggressive decay?\n2. Does larger model capacity help?\n3. What's the optimal training duration?\n4. How does generalization gap change across experiments?"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}